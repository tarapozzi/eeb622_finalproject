---
title: "Final Paper (DRAFT)"
author: "Tara Pozzi"
date: "4/14/2020"
output:
  word_document: default
bibliography: riskperception.bib
citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(psych)
library(pastecs)
library(ggplot2)
library(ggplotgui)
library(rstanarm)
library(loo) 
library(dplyr)
library(tidybayes)
```
## Introduction ##

Risk perception is a fundamental way to characterize a person’s intuitive risk judgement and allows for the identification, characterization, and quantification of risk.  Past literature has argued that those with higher risk perception are more likely to take caution and prepare for natural hazards such as floods, fires, earthquakes, etc. [@vinh_hung2007]. However, several studies have found a lack of correlation between high risk perception’s effect on protective behavioral actions towards natural disasters; which Wachinger et al. identities this unexpected response as the “Risk Perception Paradox.” [-@wachinger2013] This research project will dive into this paradox to determine if risk perception is or isn't reflected in a flood risk manager's decision to adopt a new technology called Light Detection and Ranging (LiDAR). Lidar is a laser-based survey instrument that captures a high-resolution spatial image of the earth's surface (e.g. 1-meter resolution). 

There are several factors that determine a person’s risk perception. The first area is direct experiences which have been found to have an amplificatory effect on risk perception; this includes an individual's personal experiences with natural hazard events (the largest effect) and stakehold in the community such as home ownership [@lujala2015]. The second important area of influence is trust both for scientific technology and authorities to ensure that they are being used to protect against natural hazards [@wachinger2013; @viglione2014]. This has been researched at the layperson level, however I am also curious if it holds true at the authority level. Specifically, I am interested in an individual's trust in science and the federal government for help in flood risk management. Trust in science will assess whether the person trusts the scientific information that is being provided to them about how to best manage their flood risk. Trust in the federal government will specifically assess if the person trusts that the government has the best interest of the community in mind when providing help with flood risk management. Numerous studies have showed the crucial role trust plays in building innovation, adaptive capacity, and resilience into a system, whether that system in ecological, technological, or social [@chapin2010; @luo2010; @viglione2014]. 

The third important area that could play an effect in a person’s risk perception is their belief in the potential for increased size and frequency of natural hazard events in their community. There are mixed results on the effect of increased frequency and size of events on an individual’s risk perception [@wachinger2013]. Some studies have found there to be an significant impact from a person’s environmental beliefs and their behavioral intentions [@oconnor1999]. I am curious if there is a correlation between these environmental beliefs and their risk perception which could consequently then affect their behavioral intentions. Lastly, demographics play a significant role in risk perception and will be taken into account during this analysis [@savage1993]

The current literature calls for a theoretical framework to quantify flood risk in a way that accurately takes into account the social dimension side risk with the inclusion of risk perceptions [@birkholz2014; @kellens2013]. This work aims to create a theoretical framework that can be replicated to assess risk perceptions for other types of hazards, as well as locations to ultimately help decrease vulnerability and increase adaptive capacity and resilience in communities. The main objectives of this work are: 

  1. Use a survey instrument to assess four predictor categories of risk perception: direct experience, trust, environmental beliefs, and demographics. In addition, identify if survey respondent uses LiDAR.
  
  2. Create a well-fit model that can accurately describe the relationship between risk perception and LiDAR use. 

## Hypotheses ##

Hypothesis 1 (H1): Flood risk managers with greater perceived risk of damaging floods are more likely to adopt LiDAR. 

Null: Perceived risk does not affect adoption of LiDAR in flood risk managers. 

Hypothesis 2 (H2): Direct experiences will have a more singificant effect on LiDAR use, than other risk perceptions measures of trust, environmental beliefs, and demographics. 

Null: Direct experiences with floods will have the same affect as trust, environmental beliefs, and demographics on the adoption of LiDAR. 

## Methods ##

*Data Collection*

All of the predictors as well as the response variable will be collected by a survey instrument (Appendix A). This is a modified survey instrument that was made specifically for this project, however it is part of a larger survey that will be sent out to flood risk managers across the state of Idaho. However, I have not sent this survey out for response yet, so instead I simulated data with the exact questions that I will eventually have real data for. Within each category of risk perception, there are additional sub-variables that are outlined in Table 1. In addition, the survey asks the respondent whether they currently use lidar or not which gathers the response variable data. 

![Table 1 shows four main categories of predictors: direct experience, trust, environmental beliefs, and demographics. These four predictor categories make-up a person's risk perception. Within these four categories are several sub-variables that are potential canidates for this analysis. Direct experience includes typical community flood events, the worst community flood event, the economic cost of floods per year, and if a community has inaccurate flood maps. The trust category is made up of the respondent's trust in science and the federal government. The environmental beliefs category is made up of the respondent's climate change beliefs measured by their belief in changing number and severity of flood, as well as their community's preparedness. The fourth category is the respondent's demographics which includes age, gender, education level, and political party.](variable_chart.jpg)

*Data Simulation*

A multivariate logisic regression (with binomial distribution) was chosen to model the risk perception predictors and lidar use. The reason I chose a binomial distribution is because my response variable is yes or no in regards to LiDAR adoption. This analysis has multiple variables so I created a didatic acyclical graph (DAG) to explain how the categories of predictors may influence one another and to help identify any potential confounders. 

![Table 1 shows the DAG diagram for this model where direct experience could potentially be the confounding variable influencing environmental beliefs and trust. In addition, demographics could be influencing environmental beliefs as well as lidar use directly.](dag.jpg)
 
Starting with this DAG in mind, I began my data analysis with data simulation. The first step in this process was to specify the number of respondents and the slope estimates for each predictor value. I began with 12 predictor values and decided to narrow down the predictors due to possible confounders. Specifically, I believe there is potential for incr_no_flood and incr_sev_flood to be correlated, so for the purposes of this example I decided to leave out the incr_sev_flood variable. There is also potential for age and pol_party to be correlated, so for the purposes of this example I left out age. Lastly, I decided to leave out education in order to simplify the number of variables I have in this simulation. After removing these three predictors I was left with nine predictors. Table 2 shows the updated chart of predictors for this analysis. 

![Table 2 shows the updated four main categories of predictors: direct experience, trust, environmental beliefs, and demographics.](variable_chart2.jpg)

*Model Fitting and Comparison*

I took a Bayesian approach for this analysis and ran each model using the stam_glm function with a binomial specification. The first model I ran included all nine predictor variables. I then plotted each predictor's affect on lidar use and looked for non-zero parameters. From this, I was able to identify that severity_flood, worst_flood, and pol_party had non-zero effects on lidar use. Although, I was not able to determine exactly what the effect was due to my predictor variables being uncentered and on different scales. Next, I created two additional models to compare to the full model to determine which model had the best fit. The second model looked at severity_flood, worst_flood, and pol_party on lidar use and the third model looked at direct experience on lidar use. The three model's performances were determined through the comparison of RMSE, MAE, and LOOIC values. Afther this, the model with the lowest LOOIC value and therefore "least bad" fit was  selected and examined further to determine the individual effects of each predictor on lidar use. 

*Model interpretation and Communication*

Next, I took this model and created a counterfactual plot for each variable of interest. The plot displayed the effect of one predictor on lidar use, while the other two predictors were held at the mininum value. This was done in order to try and isolate the effect of the predictor of interst on lidar use. Next, I created two more counterfactual plots to compare each predictors isolated effect on lidar use. 
 
## Results ##

Note: I accidentally saved over my simulated data with a new dataset so my numbers are not reproducible with the code as is. However, the numbers are similar. 

Model 1 has the best fit in this analysis: 

- When the full model was run, the predictors that had a potential significant impact on lidar use were severity_flood, worst_flood, and pol_party. This is because of their minimal overlap with zero.

- Then two additional models were made, Model 1 and Model 2. The results from the model comparison show that model one is the least bad model with a LOOIC of 275.3, with model two after that with 278.2, and the full model with the highest LOOIC of 287.9. This means that the model that includes the three parameters: severity_flood, worst_flood, and pol_party has the best model fit. Model 1 therefore has the least bad predictive capacity because of LOOIC= 275.3. 

![Table 4 displays the results from model one.](mod_summary.JPG)

![Table 5 displays the results from model two.](mod2.JPG)

![Table 6 displays the results from the full model.](fullmod.JPG)

- Model 1 has a RMSE = 0.628 and MAE = 0.39 which are reasonable variations from the mean for lidar use.  

Model 1 results: 

- The results summary shows that the values for mcse (all equal to 0), Rhat (all equal to 1), and n_eff (high values) are good. 

- Intercept (in this case lidar use) has a mean of -1.5, which transformed is .18. This means that 20% of survey respondents use lidar when all predictors are zero. 

- Parameter estimates from Model 1 are: severity_flood = 0.2, worst_flood = 0.1, and pol_party = 0.2. It is hard to say if this parameter estimates support either of my hypotheses because of several reasons that are explained in the discussion portion of this report. 

- Severity_flood, worst_flood, and pol_party each have positive correlations with lidar use, however in order to determine their individual effect on lidar use a counterfactual plot needs to be made. Variables are each on a different scale so they can't be directly compared in a plot all together. 
  
  - The following are the counterfactual plots, however each one didn't display the expected effect of the parameter on lidar use based on how I set the parameters. Despite this, I made the plots show what my current analysis is resulting with:  

![Figure 1 shows a counterfactural plot where worst_flood and pol_party are held constant, severity_flood results in a mean lidar use of 0.54 and mean severity_flood of 3, meaning moderate flood severity. ](cf1.jpg)

![Figure 2 shows a counterfactural plot where severity_flood and pol_party are held constant, worst_flood results in a mean lidar use of 0.54 and the mean worst_flood of 3, meaning moderate was most commonly the worst flood a community experienced.](cf2.png)
 
![Figure 3 shows a counterfactual plot where worst_flood and severity_flood are held constant, pol_party results in a mean lidar use of 0.54 and the mean political party of 2.5, but since this is a categorical question it doesn't make sense to look at this parameter value in this way. Based on a histogram for this predictor, category 1 and 4 had the largest frequency of respondents. Category 1 was democrat and category 4 was other.](cf3.jpg)
  

## Discussion ##

- I ended up having quite a few complications with my model. Primarily, I was unable to recover the effects I set in the simulation. I think that this is because I have nine variables and each one was set with a small slope and therefore had a small effect. This in turn made it hard for the model to reflect the effects I was expecting.

  - Next time I run this analysis, I want to limit the number of initial variables to five. Then I want to set the effect for two or three variables to larger slopes in hopes that my model will show this effects more clearly. 

- The counterfactual plots in my analysis all returned a similar effect between the predictor and lidar use. I think that my analysis returned this because of the interplay between small slopes and categorical nature of my data. For instance, both severity_flood and pol_party had a slope of .005 where as worst_flood was .02 however all three predictors returned a similar effect on lidar use but they have different number of categories. More specifically, the code for sequencing each predictor in the counterfactual plot has 200 random draws between 1 and 5; this is a very limited range and perhaps the pattern of the data is not able to emerge fully due to this limited range.

  - I could try plotting the effect of my predictors on lidar use in a blox plot where the categorical component of the data may be easier to view. 
  
  - I also think if I try larger slopes in the simulation part of the code, then I should be able to recover the slopes more clearly. 
  
- In addition to these concerns, I also want to find a way to account for ordered categorical data in my analysis. I am going to read McElreath's Chapter 12 about how to incorporate this in my model. 

- Overall, I don't think I was able to recover any meaningful results from this analysis however I was able to establish a methodology for looking at my data and now have a framework in place to run an analysis that will return meaningful results once I change a few key inputs! 

  
-----------------

## Appendix ##

# Apendix A #

**Survey Instrument**

Q1. Light Detection and Ranging (LiDAR) is a laser-based technology that provides a detailed data map of bare earth, canopy, and other model’s of the earth’s surface. Do you currently use LiDAR?

o	Yes 
o	No 


Q2. How severe are the consequences of flooding in your community, typically?

o	Minor-- no disruption of affected area 
o	Minimal-- short term minor economic consequences. Relocation and evacuation are not normally necessary. 
o	Moderate-- affected areas are disrupted; some areas evacuated or not habitable; dollar losses small but of consequence to those impacted; roads closed for short periods.  
o	Significant-- affected areas are essentially shut down; homes and/or basements flooded; economic losses are significant; requires temporary relocation of some; roads close for several hours; community infrastructure damaged.  
o	Disastrous-- equivalent to a major riverine flood; major community disruption; temporary relocation of many in affected areas; severe economic losses in affected areas; break up of social cohesion 



Q3. What is the worst consequence of flooding your community has experienced?

o	Minor-- no disruption of affected area 
o	Minimal-- short term minor economic consequences. Relocation and evacuation are not normally necessary. 
o	Moderate-- affected areas are disrupted; some areas evacuated or not habitable; dollar losses small but of consequence to those impacted; roads closed for short periods. 
o	Significant-- affected areas are essentially shut down; homes and/or basements flooded; economic losses are significant; requires temporary relocation of some; roads close for several hours; community infrastructure damaged. 
o	Disastrous-- equivalent to a major riverine flood; major community disruption; temporary relocation of many in affected areas; severe economic losses in affected areas; break up of social cohesion 


Q4. Do you think your community's floodplain maps accurately reflect flood risk?

o	Yes 
o	No 


Q5. In the future, do you think the number of flood events (of any level) in your community will increase, decrease, or stay the same as the current average?

o	Increase 
o	Decrease 
o	Stay the same 

Q6. In the future, do you think the severity of flood damage in your community will increase, decrease, or stay the same as the current averages?

o	Increase 
o	Decrease 
o	Stay the Same 

Q7. If you had to say, is your community prepared for a significant flood event?

o	Completely prepared 
o	Somewhat prepared 
o	Neither prepared nor unprepared 
o	Somewhat unprepared 
o	Completely unprepared 

Q8. What gender do you identify with?

o	Male 
o	Female 
o	Other 

Q9. What is your age?

o	Less than 20 years 
o	20-29 years 
o	30-39 years 
o	40-49 years 
o	50+  years 

Q10. What is the highest level of education you have completed?

o	Some high school 
o	High school diploma 
o	College education, did not graduate  
o	College education, Associates degree 
o	College education, Bachelor’s degree 
o	Advanced degree (MA, JD, MBA, PhD) 

Q11. If you are registered with a political party, in which one are you registered?
o	Democrat 
o	Independent 
o	Republican 
o	Other: ________________________________________________

Q12. How much do you trust or distrust science as a source of information to improve flood risk management?

o	Strongly trust 
o	Somewhat trust 
o	Neither trust nor distrust 
o	Somewhat distrust 
o	Strongly distrust 

Q13. How much do you trust or distrust the federal government's intent with flood risk management (i.e. data collection, accurate mapping, floodplain modeling, flood insurance)?

o	Strongly trust 
o	Somewhat trust 
o	Neither trust nor distrust 
o	Somewhat distrust 
o	Strongly distrust 

# Appendix B# 

**Model Methods & Workflow**

*Create simulated data*
```{r sim data, include=TRUE}
set.seed(200)
N=200 # number of survey respondents

## 1) Set the intercept ##

intercept=0 ## mean value of lidar use when risk perception equals zero?

## 2) Set the predictor variables ##
severity_flood= sample(1:5, N, replace=TRUE) # range of severity of floods, 1 being minor and 5 being disastrous
worst_flood= sample(1:5, N, replace=TRUE) # range of severity of floods, 1 being minor and 5 being disastrous
currentmap= sample(1:2, N, replace=TRUE) # questions asks if respondents thinks their floodmaps accurately reflect their flood risk: 1 means no and 2 means yes
incr_no_flood= sample(1:3, N, replace=TRUE) # this questions asks if respondents think the number of floods in their community is changings: 1 means increase, 2 means decrease, 3 means stay the same
incr_sev_flood= sample(1:3, N, replace=TRUE) # this questions asks if respondents think the severity of floods in their community is changings: 1 means increase, 2 means decrease, 3 means stay the same
prepared= sample(1:5, N, replace=TRUE) #this questions asks if respondents think their community is preparead for a flood event: 1 means completely prepared and 5 means completely unprepared
gender= sample(1:3, N, replace=TRUE) # gender for this is: 1 male, 2 female, and 3 other
age= sample(1:5, N, replace=TRUE) # age is categorical: where 1 is less than 20 years, 2:20-29 years, 3:30-39 years, 4:40-49 years, 5:50+ years
education= sample(1:6, N, replace=TRUE) # this is the level of education of respondent: 1: some high school, 2:high school diploma, 3: college education, did not garduate, 4: associate's degree, 5: bachelor's degreee, and 6: advanced degree
pol_party= sample(1:4, N, replace=TRUE) # there are options for the respondent to choose: 1: democrat, 2: independent, 3: republican, 4: other
science_trust= sample(1:5, N, replace=TRUE) # this question asks if respondent trusts science: 1: strongly trust to 5: strongly distrust
gov_trust= sample(1:5, N, replace=TRUE) # this question asks if respondent trusts government: 1: strongly trust to 5: strongly distrust

## 3) Potential correlations in data ##

# there is potential for incr_no_flood and incr_sev_flood to be correlated, so for the purposes of this example I am going to leave out the incr_sev_flood variable
# there is potential for age and pol_party to be correlated, so for the purposes of this example I am going to leave out age
# in addition, I am going to leave out education in order to simplify the number of variables I have in this simulation. 

## Simulating the response based on these variables ##
b1=.5
b2=0
b3=0
b4=0
#b5= 0
b6=0
b7=0
#b8= 0
#b9= 0
b10=.005
b11=.01
b12=.01
p <- intercept+(b1*severity_flood)+(b2*worst_flood)+(b3*currentmap)+(b4*incr_no_flood)+(b6*prepared)+(b7*gender)+(b10*pol_party)+(b11*science_trust)+(b12*gov_trust)
pr <- plogis(p)

## 4) Set the response variable ##

lidaruse <- rbinom(200,1,pr)

## 5) Combine data into dataframe ##

sim.survey <- data.frame(lidaruse, severity_flood, worst_flood, currentmap, incr_no_flood, prepared, gender, pol_party, science_trust, gov_trust)

write.csv(sim.survey, "sim_survey_results.csv")
```

*Fit a model to the simulated data*

```{r model fit, include=TRUE}
# bring in the simulated data
sim.survey <- read.csv("sim_survey_results.csv")
# run the first model
sim.survey.fullmod <- stan_glm(lidaruse~ severity_flood+ worst_flood+ currentmap+ incr_no_flood+ prepared+ gender+ pol_party+ science_trust+ gov_trust, data=sim.survey, family="binomial")
# plot model
plot(sim.survey.fullmod, pars="beta")
```

*Create additional models for comparison*

```{r create additional models, include=TRUE}
sim.survey.mod1 <-  stan_glm(lidaruse~severity_flood+worst_flood+pol_party, data=sim.survey, family="binomial")

sim.survey.mod2 <- stan_glm(lidaruse~severity_flood+ worst_flood+ currentmap, data=sim.survey, family="binomial")
```

```{r model compare, include=TRUE}
# look at the summary of model outputs
summary(sim.survey.fullmod)
summary(sim.survey.mod1)
summary(sim.survey.mod2)
# compare paramter estiamtes
plot(sim.survey.fullmod, pars="beta")
plot(sim.survey.mod1, pars="beta")
plot(sim.survey.mod2, pars="beta")
# compare the information criteria (loo for bayesian) to see which model loses the least amount of "realness"
loo_compare(loo(sim.survey.fullmod), loo(sim.survey.mod1), loo(sim.survey.mod2)) # loo compares all you to easily view the values relative to one another
```

```{r rmse & mae, include=TRUE}
# RMSE function:
rmse <- function(y, ypred) {
  rmse = sqrt(mean((y - ypred)^2))
  return(rmse)
}

#MAE function:
mae <- function(y, ypred) {
  mae = (mean(abs(y - ypred)))
  return(mae)
}

# Identify model's predicted yhat:
yhat.full <- posterior_predict(sim.survey.fullmod) 
yhat.full <- apply(yhat.full, 2, median) 
yhat.1 <- posterior_predict(sim.survey.mod1) 
yhat.1 <- apply(yhat.1, 2, median) 
yhat.2 <- posterior_predict(sim.survey.mod2) 
yhat.2 <- apply(yhat.2, 2, median) 
## Find the residual mean squared error and Mean Absolute Error (MAE) for the model's fit (in sample prediction)? I am running for all three models to see how it changes
rmse(sim.survey$lidaruse, yhat.full)
mae(sim.survey$lidaruse, yhat.full)
rmse(sim.survey$lidaruse, yhat.1)
mae(sim.survey$lidaruse, yhat.1)
rmse(sim.survey$lidaruse, yhat.2)
mae(sim.survey$lidaruse, yhat.2)
```

*Make counterfactual plots to determine the effect of each predictor in Model 1*

```{r flood severity counterfactual, include=TRUE}
# we want make a graph that shows the effect of severity_flood on lidar use while holding worst_flood and pol_party at their minimums
## Make a sequence of flood severity
sev.flood.gradient <- round(rep(seq(min(sim.survey$severity_flood),
                    max(sim.survey$severity_flood),length.out=200),1)) 

worst.flood.min <- min(sim.survey$worst_flood) # min flood value
pol.party.min <- min(sim.survey$pol_party) # min political affiliation

preds.sev <- add_fitted_draws(sim.survey.mod1, 
                          newdata=data.frame(sev.flood.g=sev.flood.gradient,
                                             worst.flood.m=worst.flood.min, # this is the min value of worst flood 
                                             pol.party.m=pol.party.min ), # this is the min value of political party affiliation 
                          re_formula=NA,
                          draws = 200,  	type="response")

#This line loads the original data (actual collected points)
ggplot(preds.sev, aes(x=sev.flood.g,  
                  y=.value)) +  
  stat_lineribbon(.width = c(0.5, 0.95)) +   
  scale_fill_brewer(palette = "Greys") + 
  labs(y="Lidar Use", x = "Effect of community flood severity") +
  geom_point(data=sim.survey, aes(x=severity_flood,y=lidaruse)) +
  ggtitle("The effect of community flood severity on the number of LiDAR users") +
  theme_bw()  

mean(preds.sev$.value)
mean(sim.survey$severity_flood)
``` 


```{r worst flood counterfactual}
# we want make a graph that shows the effect of worst_flood on lidar use while holding sev_flood and pol_party at their minimums

worst.flood.gradient <- round(rep(seq(min(sim.survey$worst_flood),
                    max(sim.survey$worst_flood),length.out=200),1)) 

sev.flood.min <- min(sim.survey$severity_flood)
pol.party.min <- min(sim.survey$pol_party)

preds.worst <- add_fitted_draws(sim.survey.mod1, 
                          newdata=data.frame(worst.flood.g=worst.flood.gradient,
                                             sev.flood.m=sev.flood.min, 
                                             pol.party.m=pol.party.min ), 
                          re_formula=NA,
                          draws = 200,  	type="response")

#This line loads the original data (actual collected points)
ggplot(preds.worst, aes(x=worst.flood.g,  
                  y=.value)) +  
  stat_lineribbon(.width = c(0.5, 0.95)) +   
  scale_fill_brewer(palette = "Greys") + 
  labs(y="Lidar Use", x = "Effect of the worst flood a community has seen") +
  geom_point(data=sim.survey, aes(x=worst_flood,y=lidaruse)) +
  ggtitle("The effect of the worst flood a community has seen on the number of LiDAR users") +
  theme_bw()   

mean(preds.worst$.value)
mean(sim.survey$worst_flood)
``` 
 
```{r political party counterfactual}
# we want make a graph that shows the effect of pol_party on lidar use while holding worst_flood and sev_flood at their minimums
## Make a sequence of flood severity
pol.party.gradient <- round(rep(seq(min(sim.survey$pol_party),
                    max(sim.survey$pol_party),length.out=200),1)) 

preds.pol <- add_fitted_draws(sim.survey.mod1, 
                          newdata=data.frame(pol.party.g=worst.flood.gradient,
                                             sev.flood.m=sev.flood.min, 
                                             worst.flood.m=worst.flood.min ), 
                          re_formula=NA,
                          draws = 200,  	type="response")

#This line loads the original data (actual collected points)
ggplot(preds.pol, aes(x=pol.party.g,  
                  y=.value)) +  
  stat_lineribbon(.width = c(0.5, 0.95)) +   
  scale_fill_brewer(palette = "Greys") + 
  labs(y="Lidar Use", x = "Effect of political party association") +
  geom_point(data=sim.survey, aes(x=pol_party,y=lidaruse)) +
  ggtitle("The effect of policial party association on the number of LiDAR users") +
  theme_bw()  

mean(preds.pol$.value)
mean(sim.survey$pol_party)
``` 

```{r}
hist(sim.survey$pol_party) # to help clarify political party association
```

-----------------
# References
